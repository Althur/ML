{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos nas aulas anteriores modelos lineares de regressão, onde o valor do *target* era modelado como uma combinação linear de atributos (incluindo o termo constante) mais um termo de erro $\\varepsilon$ com média zero:\n",
    "\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n + \\varepsilon = \n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_2 & \\cdots & x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix} + \\varepsilon\n",
    "= \\mathbf{x}^{T} \\theta + \\varepsilon\n",
    "$$\n",
    "\n",
    "Nosso modelo ajustado era então $\\hat{y} = h(\\mathbf{x}, \\theta_{opt}) = \\mathbf{x}^{T} \\theta_{opt}$ onde $\\theta_{opt} = \\arg \\min_{\\hat{\\theta}} \\text{MSE}(\\hat{\\theta})$ para \n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{\\theta}) = \\frac{1}{m} \\sum_{i = 1}^{m} \\left( y_i - h(\\mathbf{x}_i, \\hat{\\theta}) \\right)^2 = \\frac{1}{m} \\sum_{i = 1}^{m} \\left( y_i - \\mathbf{x}_i^{T}  \\hat{\\theta} \\right)^2\n",
    "$$\n",
    "\n",
    "As vezes, um modelo de regressão pode ser adaptado para um modelo de classificação e vice-versa. Este é o caso da ***regressão logística***, que apesar do nome *não é um método de regressão*, mas sim um método de classificação!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O problema: classificação binária\n",
    "\n",
    "Suponha que tenhamos os seguintes dados que relacionam o número de horas de estudo de um aluno com o resultado de uma prova (0: não passou; 1: passou):\n",
    "\n",
    "| horas| passou? |\n",
    "|---|---|\n",
    "| 3.75 | 0.0 |\n",
    "| 9.51 | 1.0 |\n",
    "| 7.32 | 0.0 |\n",
    "| 5.99 | 1.0 |\n",
    "| 1.56 | 0.0 |\n",
    "| 1.56 | 0.0 |\n",
    "| 0.58 | 0.0 |\n",
    "| 8.66 | 1.0 |\n",
    "| 6.01 | 1.0 |\n",
    "| 7.08 | 1.0 |\n",
    "| 0.21 | 0.0 |\n",
    "| 9.70 | 1.0 |\n",
    "| 8.32 | 0.0 |\n",
    "| 2.12 | 0.0 |\n",
    "| 1.82 | 0.0 |\n",
    "| 1.83 | 0.0 |\n",
    "| 3.04 | 0.0 |\n",
    "| 5.25 | 1.0 |\n",
    "| 4.32 | 0.0 |\n",
    "| 2.91 | 0.0 |\n",
    "| 6.12 | 1.0 |\n",
    "| 1.39 | 0.0 |\n",
    "| 2.92 | 1.0 |\n",
    "| 3.66 | 1.0 |\n",
    "| 4.56 | 0.0 |\n",
    "| 7.85 | 1.0 |\n",
    "| 2.00 | 0.0 |\n",
    "| 5.14 | 0.0 |\n",
    "| 5.92 | 0.0 |\n",
    "| 0.46 | 0.0 |\n",
    "\n",
    "Eis um gráfico para ajudar a visualizar esses dados:\n",
    "\n",
    "![passou ou não](alunos.png \"Resultado do teste versus número de horas de estudo\")\n",
    "\n",
    "Parece que se um aluno não estuda não passa, e se estuda bastante passa. E no meio do caminho? Como estimar a chance de que o aluno passe se estudar $7$ horas, por exemplo? Parece que precisamos de uma função interpoladora aqui! Existem várias opções de função interpoladora, vamos estudar uma delas: a **função logística**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função logística\n",
    "\n",
    "Para adaptar a regressão linear para a regressão logística (que não é regressão, mas sim um método de classificação), precisamos da *função logística*:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Esta função se comporta da seguinte maneira:\n",
    "\n",
    "- Para valores muito negativos de $x$ temos $\\lim_{x \\rightarrow -\\infty} \\sigma(x) = 0$ pois o denominador da fração vai para infinito.\n",
    "\n",
    "- Para valores muito positivos de $x$ temos $\\lim_{x \\rightarrow \\infty} \\sigma(x) = 1$ pois $e^{-x}$ vai para zero.\n",
    "\n",
    "- Para $x = 0$ temos $\\sigma(0) = 0.5$\n",
    "\n",
    "Eis a cara dessa função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gera valores da função logística entre -10 e 10.\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Plota a função logística.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Plota linhas auxiliares só para visualizar melhor.\n",
    "plt.plot([0, 0], [0, 1], 'k-')\n",
    "plt.plot([-10, 10], [0.0, 0.0], 'k-')\n",
    "plt.plot([-10, 10], [0.5, 0.5], 'k--')\n",
    "plt.plot([-10, 10], [1.0, 1.0], 'k--')\n",
    "\n",
    "# Resto do gráfico: titulo, labels, etc.\n",
    "plt.title('Função logística')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$y = \\sigma(x)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função logística tem um formato *sigmoide* (ou seja, em forma de \"s\").\n",
    "\n",
    "---\n",
    "\n",
    "**Atividade:** Como você faria (matematicamente) para:\n",
    "\n",
    "- Deslocar a função logística para a direita?\n",
    "\n",
    "- Aumentar a largura da \"zona de transição de zero para um\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tentar ajustar uma função logística aos nossos dados dos alunos - essa é a base da regressão logística (que não é regressão, mas sim um método de classificação). Na figura abaixo esse ajuste está feito - vamos aprender como fazê-lo logo em seguida.\n",
    "\n",
    "![passou ou não prob](alunos_prob.png \"Probabilidade de aprovação versus número de horas de estudo\")\n",
    "\n",
    "Se tivermos que decidir se achamos que um aluno passa ou não de acordo com o número de horas de estudo deste, o melhor é adotar uma *regra de decisão* do tipo:\n",
    "\n",
    "- $\\hat{p}$ (probabilidade estimada) maior ou igual que $50\\%$: acho que passa.\n",
    "\n",
    "- $\\hat{p}$ menor que $50\\%$: acho que não passa.\n",
    "\n",
    "Temos agora um classificador de aluno! Eis o gráfico deste classificador em cima dos dados:\n",
    "\n",
    "![passou ou não class](alunos_class.png \"Vai passar ou não? versus número de horas de estudo\")\n",
    "\n",
    "Essa é a idéia da regressão logística (que não é regressão, mas sim um método de classificação). Vamos estudar agora mais a fundo esse modelo, e como fazer para descobrir os parâmetros da função logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Atividade**\n",
    "\n",
    "- Qual o *precision*, *recall* e acurácia deste exemplo?\n",
    "\n",
    "- Se eu quisesse garantir uma chance de aprovação de mais de $80\\%$, quantas horas um aluno deveria estudar? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de regressão logística (que não é regressão, mas sim um método de classificação)\n",
    "\n",
    "Em um modelo de regressão linear estamos prevendo o valor da variável dependente. Em uma regressão logística (que não é regressão, mas sim um método de classificação), o que estamos tentando prever? Como se trata de um método de classificação, estamos tentando prever a classe $y$ de um objeto de atributos $\\mathbf{x}$. Esta classe deverá ser binária: zero ou um, negativo ou positivo. A regressão logística (que não é regressão, mas sim um método de classificação) atinge este objetivo da seguinte forma:\n",
    "\n",
    "- Para um conjunto de parâmetros $\\theta$, calcule a probabilidade (segundo o modelo) de que o objeto de atributos $\\mathbf{x}$ seja da classe positiva:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\sigma(\\mathbf{x}^{T} \\theta)\n",
    "$$\n",
    "\n",
    "Esta será a nossa função de decisão!\n",
    "\n",
    "(Como encontrar $\\theta$? Esse é o objetivo do algoritmo de treinamento, que vamos ver mais abaixo.)\n",
    "\n",
    "- Determine a classe do objeto usando o threshold $0.5$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\left\\{ \n",
    "\\begin{matrix}\n",
    "0, \\text{ se } \\hat{p} < 0.5 \\\\\n",
    "1, \\text{ se } \\hat{p} \\ge 0.5 \\\\\n",
    "\\end{matrix}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Usando a nomenclatura da função indicadora: $\\hat{y} = I_{x \\ge 0.5}(\\sigma(\\mathbf{x}^{T} \\theta))$ .\n",
    "\n",
    "## Função de custo\n",
    "\n",
    "Para obter o valor ótimo dos parâmetros $\\theta$ de um modelo de regressão logística temos que definir uma função de custo. Existem inúmeras possibilidades: basta escolher uma estratégia que penalize os erros e/ou valorize os acertos.\n",
    "\n",
    "Uma opção bastante conveniente (veremos depois porque) de função de custo para um dado objeto $(\\mathbf{x}, y)$ e um vetor de parâmetros $\\theta$ é a seguinte:\n",
    "\n",
    "- A probabilidade predita é $\\hat{p} = h(\\mathbf{x}, \\theta) = \\sigma(\\mathbf{x}^T \\theta)$\n",
    "\n",
    "- Se a classe real $y$ for 1, a função de custo será $-log(\\hat{p})$\n",
    "    \n",
    "- Se a classe real $y$ for 0, a função de custo será $-log(1 - \\hat{p})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "**Atividade**\n",
    "\n",
    "Explique porque esta é uma função de custo que funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de custo completa, para todas as amostras, é o custo médio por amostra:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^{m} \\left[y_i \\log(\\hat{p}) + (1 - y_i) (\\log(1 - \\hat{p})) \\right]\n",
    "$$\n",
    "\n",
    "Diferentemente do caso da regressão linear, aqui não temos uma solução fechada como a equação normal. Só nos resta o *gradient descent*. A boa notícia é que com essa função de custo as derivadas parciais são surpreendentemente simples:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \n",
    "\\frac{1}{m} \n",
    "\\sum_{i = 1}^{m} \n",
    "\\left[ \\sigma\\left( \\mathbf{x}^T \\theta_i \\right) - y_i \\right]\n",
    "\\mathbf{x}_{i,j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris: o verdadeiro \"Hello, world!\" dos modelos preditivos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename']\n"
     ]
    }
   ],
   "source": [
    "print(list(iris.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**\n",
    "\n",
    "Construa um classificador por regressão logística para separar as flores do tipo 'Iris Virginica' das demais usando as características 'petal length (cm)' e 'petal width (cm)'. Como resultado final, apresente:\n",
    "\n",
    "- Acurácia do classificador no conjunto de testes.\n",
    "- Curva ROC e respectiva área.\n",
    "- Um diagrama ilustrando a probabilidade da classe positiva. \n",
    "    - Dica: veja https://matplotlib.org/gallery/images_contours_and_fields/contour_demo.html\n",
    "\n",
    "Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão linear é bacana, mas só serve para classificação binária. Se nosso problema for de classificação multiclasse, como proceder? Uma alternativa é recorrer às técnicas de \"One-Versus-One\" e \"One-Versus-All\" vista nas aulas passadas.\n",
    "\n",
    "Mas temos uma alternativa melhor aqui: podemos generalizar a técnica de regressão linear para a situação de várias classes: esta é a regressão linear multiclasse, ou regressão *softmax*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função softmax\n",
    "\n",
    "A regressão logística (que não é regressão mas sim classificação) consiste em ajustar aos dados (via minimização da função de custo) uma curva logística. Isso pode ser quebrado em duas partes:\n",
    "\n",
    "- Calcular o *score* do objeto, que é $\\mathbf{x}^T \\theta$\n",
    "\n",
    "- Passar o *score* pela função logística: $\\hat{p} = \\sigma(score)$\n",
    "\n",
    "E se tivéssemos várias classes? Uma possibilidade é a seguinte:\n",
    "\n",
    "- Para cada classe, calcule um *score* desta classe para o objeto: $s_k = \\mathbf{x}^T \\theta_k$. Note que agora temos um vetor de parâmetros $\\theta_k$ por classe $k$.\n",
    "\n",
    "- Normalize esses *scores* com o auxílio da *função softmax*:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k = \\sigma(s(\\mathbf{x}))_k = \\frac{\\exp\\left( s_k(\\mathbf{x}) \\right)}{\\sum_{j=1}^{K} \\exp\\left( s_j(\\mathbf{x}) \\right)}\n",
    "$$\n",
    "\n",
    "A classe atribuida ao objeto será então $\\arg \\max_k \\sigma(s(\\mathbf{x}))_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropia cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de custo da regressão *softmax* é muito parecida com a função de custo da regressão logística binária:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m} \\left[ - \\sum_{k = 1}^{K} [y_i = k] \\log(\\hat{p}_k\\left( \\mathbf{x}_i \\right)) \\right]\n",
    "$$\n",
    "\n",
    "onde a notação $[y_i = k]$ vale $1$ se a condição é verdadeira, e $0$ caso contrário. Esta notação chama-se \"colchetes de Iverson\" (*\"Iverson's brackets\"*).\n",
    "\n",
    "Esta expressão é a *entropia cruzada* entre $\\mathbf{y}$ e $\\mathbf{\\hat{p}}$. Esta é uma medida que vem da teoria da informação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Repita a atividade de classificação do dataset 'Iris' usando apenas as características 'petal length (cm)' e 'petal width (cm)'. Como resultado final, apresente:\n",
    "\n",
    "- Acurácia do classificador no conjunto de testes.\n",
    "- Diagramas ilustrando a probabilidade para cada classe\n",
    "    - Dica: veja https://matplotlib.org/gallery/images_contours_and_fields/contour_demo.html\n",
    "\n",
    "Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade (para casa):** Repita a atividade anterior usando todas as quatro características originais. Qual o aumento de desempenho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
